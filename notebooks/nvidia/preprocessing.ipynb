{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f120fb8e-0359-4db3-8298-05228fe01518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from category_encoders) (0.14.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m187.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: numpy, category_encoders\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [category_encoders]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.252.0 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.16.2 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed category_encoders-2.8.1 numpy-2.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade 'scikit-learn' pandas numpy category_encoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ef9f6b-a29e-4706-896f-14b4a2429917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_21915/3512803361.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 53, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numexpr/__init__.py\", line 26, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import os, json, time, getpass, subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from category_encoders import BinaryEncoder\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df75fad5-a8b0-45b6-b597-e8d7a562bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"sagemaker-us-east-1-176843580427\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28cb692-009f-42c5-b418-7d0f2c9ffd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-176843580427/datasets/ieee-fraud-detection/train_identity.csv to datasets/ieee-fraud-detection/train_identity.csv\n",
      "download: s3://sagemaker-us-east-1-176843580427/datasets/ieee-fraud-detection/train_transaction.csv to datasets/ieee-fraud-detection/train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp \\\n",
    "    --recursive \\\n",
    "    s3://{bucket_name}/datasets/ieee-fraud-detection/ \\\n",
    "    ./datasets/ieee-fraud-detection/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf1862d-bfa1-49c2-8753-0bfb958a0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\t\t   preprocessing.ipynb\t training-job.ipynb\n",
      "download-upload-ecr.ipynb  processed\n",
      "lost+found\t\t   training_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ce4bf2-a27b-4ba5-9cd1-b2fea5033413",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      " RAW_DATA_PATH: ./datasets/ieee-fraud-detection/\n",
      " PROCESSED_DATA_PATH: ./processed/ieee-fraud-detection/\n",
      " MODEL_OUTPUT_PATH: ./trained_models/ieee-fraud-detection/\n",
      " BUCKET PREPROCESS PATH s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/\n"
     ]
    }
   ],
   "source": [
    "BASE_DRIVE = \"./\"\n",
    "\n",
    "RAW_DATA_PATH = os.path.join(BASE_DRIVE, 'datasets/ieee-fraud-detection/')          # put train_transaction.csv & optional train_identity.csv here\n",
    "PROCESSED_DATA_PATH = os.path.join(BASE_DRIVE, 'processed/ieee-fraud-detection/')   # output used by Docker\n",
    "S3_PREPROCESS_DATA_PATH = os.path.join(\"s3://\", bucket_name, 'processed/ieee-fraud-detection/')   # output used by Docker\n",
    "MODEL_OUTPUT_PATH = os.path.join(BASE_DRIVE, 'trained_models/ieee-fraud-detection/')# models written by the container\n",
    "\n",
    "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Paths:\")\n",
    "print(\" RAW_DATA_PATH:\", RAW_DATA_PATH)\n",
    "print(\" PROCESSED_DATA_PATH:\", PROCESSED_DATA_PATH)\n",
    "print(\" MODEL_OUTPUT_PATH:\", MODEL_OUTPUT_PATH)\n",
    "print(\" BUCKET PREPROCESS PATH\", S3_PREPROCESS_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7ee69a-bf32-46f5-8df5-adb92e4c83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Load raw data and split (time-based)\n",
    "# ---------------------------\n",
    "csv_path = Path(RAW_DATA_PATH) / 'train_transaction.csv'\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"Place `train_transaction.csv` under {RAW_DATA_PATH} before running this cell\")\n",
    "train_transaction = pd.read_csv(csv_path)\n",
    "identity_path = Path(RAW_DATA_PATH) / 'train_identity.csv'\n",
    "train_identity = pd.read_csv(identity_path) if identity_path.exists() else None\n",
    "\n",
    "# Optionally quick sample for debugging (set None for full)\n",
    "SAMPLE_SIZE = None  # e.g., 100000 for quick run\n",
    "if SAMPLE_SIZE:\n",
    "    train_transaction = train_transaction.head(SAMPLE_SIZE)\n",
    "    if train_identity is not None:\n",
    "        train_identity = train_identity[train_identity['TransactionID'].isin(train_transaction['TransactionID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0c8b91-23d5-4139-8c14-9b4a55e086c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging identity file...\n",
      "Replacing placeholder strings with NaN...\n",
      "Identified features: 171 categorical, 263 numerical\n",
      "Encoding categoricals on full dataset...\n",
      "Scaling numericals on full dataset...\n",
      "✅ Preprocessing complete on full dataset. Now performing time-based split...\n",
      "Splits: Train=413378, Test=177162\n",
      "Train fraud rate=0.035169, Test fraud rate=0.034573\n",
      "Feature columns count: 434\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Merge identity and preprocess BEFORE split\n",
    "# ---------------------------\n",
    "print(\"Merging identity file...\")\n",
    "if train_identity is not None:\n",
    "    train_merged = train_transaction.merge(train_identity, on=\"TransactionID\", how=\"left\")\n",
    "else:\n",
    "    train_merged = train_transaction.copy()\n",
    "\n",
    "print(\"Replacing placeholder strings with NaN...\")\n",
    "train_merged.replace([\"unknown\", \"Unknown\", \"UNKN\"], np.nan, inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Identify categorical and numerical features\n",
    "# ---------------------------\n",
    "categorical_features = [\n",
    "    c for c in train_merged.columns\n",
    "    if train_merged[c].dtype == \"object\" or train_merged[c].nunique() < 20\n",
    "]\n",
    "numerical_features = [\n",
    "    c for c in train_merged.columns\n",
    "    if c not in categorical_features + [\"isFraud\"]\n",
    "]\n",
    "\n",
    "print(f\"Identified features: {len(categorical_features)} categorical, {len(numerical_features)} numerical\")\n",
    "\n",
    "# ---------------------------\n",
    "# Fill missing values BEFORE split\n",
    "# ---------------------------\n",
    "for c in categorical_features:\n",
    "    train_merged[c] = train_merged[c].fillna(\"missing\")\n",
    "for c in numerical_features:\n",
    "    train_merged[c] = train_merged[c].fillna(train_merged[c].median())\n",
    "\n",
    "# ---------------------------\n",
    "# Encode categoricals BEFORE split\n",
    "# ---------------------------\n",
    "print(\"Encoding categoricals on full dataset...\")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "encoders = {}\n",
    "for c in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_merged[c] = le.fit_transform(train_merged[c].astype(str))\n",
    "    encoders[c] = le\n",
    "\n",
    "# ---------------------------\n",
    "# Scale numerical features BEFORE split\n",
    "# ---------------------------\n",
    "print(\"Scaling numericals on full dataset...\")\n",
    "scaler = StandardScaler()\n",
    "train_merged[numerical_features] = scaler.fit_transform(train_merged[numerical_features].astype(float))\n",
    "\n",
    "print(\"✅ Preprocessing complete on full dataset. Now performing time-based split...\")\n",
    "\n",
    "# ---------------------------\n",
    "# Split (time-based AFTER preprocessing)\n",
    "# ---------------------------\n",
    "train_merged = train_merged.sort_values('TransactionDT').reset_index(drop=True)\n",
    "n = len(train_merged)\n",
    "train_end = int(n * 0.7)\n",
    "test_start = train_end\n",
    "train_df = train_merged.iloc[:train_end].reset_index(drop=True)\n",
    "test_df = train_merged.iloc[test_start:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Splits: Train={len(train_df)}, Test={len(test_df)}\")\n",
    "print(f\"Train fraud rate={train_df['isFraud'].mean():.6f}, Test fraud rate={test_df['isFraud'].mean():.6f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# UTILITIES: Save functions in NGC expected structure\n",
    "# ---------------------------\n",
    "def save_gnn_data(output_dir, edges_df, node_features_df, node_labels_df, tx_range, split_name='train'):\n",
    "    base = Path(output_dir) / 'gnn' / (split_name if split_name != 'train' else 'train_gnn')\n",
    "    edge_path = base / 'edges'\n",
    "    node_path = base / 'nodes'\n",
    "    edge_path.mkdir(parents=True, exist_ok=True)\n",
    "    node_path.mkdir(parents=True, exist_ok=True)\n",
    "    edges_df.to_csv(edge_path / 'node_to_node.csv', index=False)\n",
    "    node_features_df.to_csv(node_path / 'node.csv', index=False)\n",
    "    node_labels_df.to_csv(node_path / 'node_label.csv', index=False)\n",
    "    if split_name == 'train':\n",
    "        offset_info = {'start': int(tx_range[0]), 'end': int(tx_range[1])}\n",
    "        with open(node_path / 'offset_range_of_training_node.json', 'w') as f:\n",
    "            json.dump(offset_info, f, indent=4)\n",
    "    print(f\"Saved GNN data for split={split_name} at {base}\")\n",
    "\n",
    "def save_xgb_data(output_dir, train_df, test_df, feature_cols, categorical_cols):\n",
    "    xgb_path = Path(output_dir) / 'xgb'\n",
    "    xgb_path.mkdir(parents=True, exist_ok=True)\n",
    "    cols = list(feature_cols) + ['isFraud']\n",
    "    train_df[cols].to_csv(xgb_path / 'training.csv', index=False)\n",
    "    test_df[cols].to_csv(xgb_path / 'test.csv', index=False)\n",
    "    cat_info = {'categorical_features': categorical_cols, 'feature_names': list(feature_cols)}\n",
    "    with open(xgb_path / 'feature_info.json', 'w') as f:\n",
    "        json.dump(cat_info, f, indent=4)\n",
    "    print(f\"Saved XGBoost data at {xgb_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Graph builder (same as before)\n",
    "# ---------------------------\n",
    "class GraphBuilder:\n",
    "    def __init__(self, df, card_col='card1', email_col='P_emaildomain', entity_vocab=None):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.df['tx_id'] = self.df.index\n",
    "        self.card_col = card_col\n",
    "        self.email_col = email_col\n",
    "        self.entity_vocab = entity_vocab\n",
    "\n",
    "    def build_graph(self):\n",
    "        if self.entity_vocab is None:\n",
    "            unique_cards = pd.Series(self.df[self.card_col].astype(str).unique())\n",
    "            self.card_to_id = {card: idx for idx, card in enumerate(unique_cards)}\n",
    "            unique_emails = pd.Series(self.df[self.email_col].astype(str).unique())\n",
    "            self.email_to_id = {email: idx for idx, email in enumerate(unique_emails)}\n",
    "            self.entity_vocab = {'card_to_id': self.card_to_id, 'email_to_id': self.email_to_id}\n",
    "        else:\n",
    "            self.card_to_id = self.entity_vocab['card_to_id']\n",
    "            self.email_to_id = self.entity_vocab['email_to_id']\n",
    "            before = len(self.df)\n",
    "            self.df = self.df[\n",
    "                self.df[self.card_col].astype(str).isin(self.card_to_id.keys()) &\n",
    "                self.df[self.email_col].astype(str).isin(self.email_to_id.keys())\n",
    "            ].reset_index(drop=True)\n",
    "            self.df['tx_id'] = self.df.index\n",
    "            after = len(self.df)\n",
    "            print(f\"Filtered unseen entities: before={before}, after={after} (dropped {before-after})\")\n",
    "\n",
    "        self.num_cards = len(self.card_to_id)\n",
    "        self.num_emails = len(self.email_to_id)\n",
    "        self.num_transactions = len(self.df)\n",
    "        print(f\"Graph nodes: {self.num_cards} cards, {self.num_emails} emails, {self.num_transactions} txs\")\n",
    "        return self\n",
    "\n",
    "    def create_edges(self):\n",
    "        self.df['card_id'] = self.df[self.card_col].astype(str).map(self.card_to_id)\n",
    "        self.df['email_id'] = self.df[self.email_col].astype(str).map(self.email_to_id)\n",
    "        self.df = self.df.dropna(subset=['card_id', 'email_id']).copy()\n",
    "        self.df['card_id'] = self.df['card_id'].astype(int)\n",
    "        self.df['email_id'] = self.df['email_id'].astype(int)\n",
    "        card_offset = 0\n",
    "        email_offset = self.num_cards\n",
    "        tx_offset = self.num_cards + self.num_emails\n",
    "        edges = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            card_node = card_offset + row['card_id']\n",
    "            email_node = email_offset + row['email_id']\n",
    "            tx_node = tx_offset + row['tx_id']\n",
    "            edges.extend([\n",
    "                [card_node, tx_node],\n",
    "                [tx_node, email_node],\n",
    "                [tx_node, card_node],\n",
    "                [email_node, tx_node]\n",
    "            ])\n",
    "        return pd.DataFrame(edges, columns=['src', 'dst'])\n",
    "\n",
    "# ---------------------------\n",
    "# Feature column list\n",
    "# ---------------------------\n",
    "feature_cols = categorical_features + numerical_features\n",
    "print(f\"Feature columns count: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fca664a-8647-4912-b229-a8f44a3b35d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train graph...\n",
      "Graph nodes: 12242 cards, 60 emails, 413378 txs\n",
      "Saved GNN data for split=train at processed/ieee-fraud-detection/gnn/train_gnn\n",
      "Building test graph...\n",
      "Filtered unseen entities: before=177162, after=174985 (dropped 2177)\n",
      "Graph nodes: 12242 cards, 60 emails, 174985 txs\n",
      "Saved GNN data for split=test at processed/ieee-fraud-detection/gnn/test\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# BUILD GRAPH DATA\n",
    "# ===============================\n",
    "print(\"Building train graph...\")\n",
    "train_graph = GraphBuilder(train_df).build_graph()\n",
    "train_edges = train_graph.create_edges()\n",
    "save_gnn_data(PROCESSED_DATA_PATH, train_edges, train_df[feature_cols], train_df[['isFraud']], (0, len(train_df)), split_name='train')\n",
    "\n",
    "print(\"Building test graph...\")\n",
    "test_graph = GraphBuilder(test_df, entity_vocab=train_graph.entity_vocab).build_graph()\n",
    "test_edges = test_graph.create_edges()\n",
    "save_gnn_data(PROCESSED_DATA_PATH, test_edges, test_df[feature_cols], test_df[['isFraud']], (len(train_df), len(train_df)+len(test_df)), split_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02765ee-23cf-4039-86fd-6feaaf9877bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGBoost data at processed/ieee-fraud-detection/xgb\n",
      "✅ All preprocessing, graph creation, and dataset export completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SAVE XGBOOST TRAIN/TEST FILES\n",
    "# ===============================\n",
    "save_xgb_data(PROCESSED_DATA_PATH, train_df, test_df, feature_cols, categorical_features)\n",
    "\n",
    "print(\"✅ All preprocessing, graph creation, and dataset export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373430e6-9c4c-430e-8277-10b54846d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(directory, prefix=\"\"):\n",
    "    \"\"\"Recursively prints the directory tree starting at 'directory'.\"\"\"\n",
    "    # Retrieve a sorted list of entries in the directory\n",
    "    entries = sorted(os.listdir(directory))\n",
    "    entries_count = len(entries)\n",
    "    \n",
    "    for index, entry in enumerate(entries):\n",
    "        path = os.path.join(directory, entry)\n",
    "        # Determine the branch connector\n",
    "        if index == entries_count - 1:\n",
    "            connector = \"└── \"\n",
    "            extension = \"    \"\n",
    "        else:\n",
    "            connector = \"├── \"\n",
    "            extension = \"│   \"\n",
    "        \n",
    "        print(prefix + connector + entry)\n",
    "        \n",
    "        # If the entry is a directory, recursively print its contents\n",
    "        if os.path.isdir(path):\n",
    "            print_tree(path, prefix + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c62802-bbb4-4e4a-8a96-7edd2af936d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── .ipynb_checkpoints\n",
      "├── config\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   │   └── config-checkpoint.json\n",
      "│   ├── config.json\n",
      "│   └── training_config.json\n",
      "├── gnn\n",
      "│   ├── test\n",
      "│   │   ├── edges\n",
      "│   │   │   ├── .ipynb_checkpoints\n",
      "│   │   │   │   └── node_to_node-checkpoint.csv\n",
      "│   │   │   └── node_to_node.csv\n",
      "│   │   └── nodes\n",
      "│   │       ├── .ipynb_checkpoints\n",
      "│   │       │   └── node_label-checkpoint.csv\n",
      "│   │       ├── node.csv\n",
      "│   │       └── node_label.csv\n",
      "│   └── train_gnn\n",
      "│       ├── edges\n",
      "│       │   └── node_to_node.csv\n",
      "│       └── nodes\n",
      "│           ├── node.csv\n",
      "│           ├── node_label.csv\n",
      "│           └── offset_range_of_training_node.json\n",
      "└── xgb\n",
      "    ├── feature_info.json\n",
      "    ├── test.csv\n",
      "    └── training.csv\n"
     ]
    }
   ],
   "source": [
    "print_tree(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5483e6b0-cdb7-4b64-9e68-8dc603dfc833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.434310083918007"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Create training_config.json (NGC container expects this)\n",
    "# ---------------------------\n",
    "fraud_count = int(train_df['isFraud'].sum())\n",
    "non_fraud_count = int(len(train_df) - fraud_count)\n",
    "scale_pos_weight = float(non_fraud_count) / max(1.0, float(fraud_count))\n",
    "# \"output_dir\": \"/trained_models\"   # container path -> maps to MODEL_OUTPUT_PATH on host /opt/ml/model\n",
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50dc8b86-1ab6-4843-96f4-83742cc74454",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: processed/ieee-fraud-detection/config/.ipynb_checkpoints/config-checkpoint.json to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/config/.ipynb_checkpoints/config-checkpoint.json\n",
      "upload: processed/ieee-fraud-detection/config/training_config.json to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/config/training_config.json\n",
      "upload: processed/ieee-fraud-detection/config/config.json to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/config/config.json\n",
      "upload: processed/ieee-fraud-detection/gnn/test/nodes/.ipynb_checkpoints/node_label-checkpoint.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/test/nodes/.ipynb_checkpoints/node_label-checkpoint.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/test/nodes/node_label.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/test/nodes/node_label.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/test/edges/.ipynb_checkpoints/node_to_node-checkpoint.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/test/edges/.ipynb_checkpoints/node_to_node-checkpoint.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/test/edges/node_to_node.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/test/edges/node_to_node.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/test/nodes/node.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/test/nodes/node.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/train_gnn/edges/node_to_node.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/train_gnn/edges/node_to_node.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/train_gnn/nodes/node_label.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/train_gnn/nodes/node_label.csv\n",
      "upload: processed/ieee-fraud-detection/gnn/train_gnn/nodes/offset_range_of_training_node.json to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/train_gnn/nodes/offset_range_of_training_node.json\n",
      "upload: processed/ieee-fraud-detection/xgb/feature_info.json to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/xgb/feature_info.json\n",
      "upload: processed/ieee-fraud-detection/gnn/train_gnn/nodes/node.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/gnn/train_gnn/nodes/node.csv\n",
      "upload: processed/ieee-fraud-detection/xgb/test.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/xgb/test.csv\n",
      "upload: processed/ieee-fraud-detection/xgb/training.csv to s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/xgb/training.csv\n",
      "2025-10-13 03:50:08        874 processed/ieee-fraud-detection/config/.ipynb_checkpoints/config-checkpoint.json\n",
      "2025-10-13 03:50:08        872 processed/ieee-fraud-detection/config/config.json\n",
      "2025-10-13 03:50:08        874 processed/ieee-fraud-detection/config/training_config.json\n",
      "2025-10-13 03:50:08   10849426 processed/ieee-fraud-detection/gnn/test/edges/.ipynb_checkpoints/node_to_node-checkpoint.csv\n",
      "2025-10-13 03:50:08   10849426 processed/ieee-fraud-detection/gnn/test/edges/node_to_node.csv\n",
      "2025-10-13 03:50:08     354332 processed/ieee-fraud-detection/gnn/test/nodes/.ipynb_checkpoints/node_label-checkpoint.csv\n",
      "2025-10-13 03:50:08 1036391430 processed/ieee-fraud-detection/gnn/test/nodes/node.csv\n",
      "2025-10-13 03:50:08     354332 processed/ieee-fraud-detection/gnn/test/nodes/node_label.csv\n",
      "2025-10-13 03:50:14   26124922 processed/ieee-fraud-detection/gnn/train_gnn/edges/node_to_node.csv\n",
      "2025-10-13 03:50:15 2419614275 processed/ieee-fraud-detection/gnn/train_gnn/nodes/node.csv\n",
      "2025-10-13 03:50:30     826764 processed/ieee-fraud-detection/gnn/train_gnn/nodes/node_label.csv\n",
      "2025-10-13 03:50:33         37 processed/ieee-fraud-detection/gnn/train_gnn/nodes/offset_range_of_training_node.json\n",
      "2025-10-13 03:50:33       9636 processed/ieee-fraud-detection/xgb/feature_info.json\n",
      "2025-10-13 03:50:33 1036745762 processed/ieee-fraud-detection/xgb/test.csv\n",
      "2025-10-13 03:50:40 2420441039 processed/ieee-fraud-detection/xgb/training.csv\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp --recursive {PROCESSED_DATA_PATH} {S3_PREPROCESS_DATA_PATH}\n",
    "! aws s3 ls --recursive {S3_PREPROCESS_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c366a-5574-4123-a2f2-5ed2c8d9ba67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
