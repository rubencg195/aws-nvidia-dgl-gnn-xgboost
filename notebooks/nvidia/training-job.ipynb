{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb22db-b7d8-4b75-a3d0-2beb76514027",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176843580427.dkr.ecr.us-east-1.amazonaws.com/financial-fraud-training:1.0.1\n",
      "                           PRE /\n",
      "                           PRE config/\n",
      "                           PRE datasets/\n",
      "                           PRE output/\n",
      "                           PRE processed/\n",
      "                           PRE s3:/\n"
     ]
    }
   ],
   "source": [
    "# https://docs.nvidia.com/nim/financial-fraud-training/1.0.0/configuration/config-json.html#overview-of-parameters-and-hyperparameters\n",
    "# https://github.com/NVIDIA-AI-Blueprints/financial-fraud-detection/blob/main/notebooks/financial-fraud-usage.ipynb\n",
    "# https://aws.amazon.com/ec2/instance-types/g5/\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import subprocess\n",
    "import getpass\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ---------- PARAMETERS ----------\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "ecr_image_name = \"financial-fraud-training\"\n",
    "ecr_image_tag = \"1.0.1\"\n",
    "ecr_repo_name = f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/{ecr_image_name}:{ecr_image_tag}\"\n",
    "bucket_name = f\"sagemaker-{region}-{account_id}\"\n",
    "from pathlib import Path\n",
    "import json\n",
    "# Load from repo root credentials file\n",
    "cred_paths = [Path('../../nvidia_credentials.json'), Path('../nvidia_credentials.json'), Path('nvidia_credentials.json')]\n",
    "NGC_API_KEY = None\n",
    "for p in cred_paths:\n",
    "    if p.exists():\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            NGC_API_KEY = json.load(f).get('ngc_api_key')\n",
    "        break\n",
    "if not NGC_API_KEY:\n",
    "    raise FileNotFoundError('nvidia_credentials.json not found or missing ngc_api_key. Place it at repo root with your real key.')\n",
    "scale_pos_weight = 27.434310083918007\n",
    "\n",
    "print(ecr_repo_name)\n",
    "!aws s3 ls {bucket_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d00d3-33cc-454e-a9d1-a94cecc01628",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "850103a9-1a55-470f-a7da-84cffdd28750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::176843580427:role/SageMakerExecutionRole\n"
     ]
    }
   ],
   "source": [
    "import pandas, sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile, DetailedProfilingConfig\n",
    "\n",
    "sagemaker_training_role = sagemaker.get_execution_role()\n",
    "print(sagemaker_training_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea132bb-0962-4dc6-92ae-ac88aaebc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload wrapper script to S3\n",
    "def upload_file(s3_bucket_name, s3_key, local_filepath):\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(\n",
    "        local_filepath,\n",
    "        s3_bucket_name,\n",
    "        s3_key\n",
    "    )\n",
    "    \n",
    "    s3_path = f\"s3://{s3_bucket_name}/{s3_key}\"\n",
    "    print(f\"File uploaded from {local_filepath} to: {s3_path}\")\n",
    "    !aws s3 ls {s3_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac787f54-44b4-4e11-82a0-16bb3655a67f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/',\n",
       " 's3://sagemaker-us-east-1-176843580427/output/ieee-fraud-detection/')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the training in Sagemaker\n",
    "# send training job to parameter store\n",
    "session = sagemaker.Session()\n",
    "\n",
    "region = session.boto_region_name\n",
    "\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "\n",
    "#ssm_client.put_parameter(Name=\"/triton/model\", Value=training_job_name, Type=\"String\", Overwrite=True)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "BASE_DRIVE = \"./\"\n",
    "RAW_DATA_PATH = os.path.join(BASE_DRIVE, 'datasets/ieee-fraud-detection/') \n",
    "PROCESSED_DATA_PATH = os.path.join(BASE_DRIVE, 'processed/ieee-fraud-detection/')   # output used by Docker\n",
    "S3_PREPROCESS_DATA_PATH = os.path.join(\"s3://\", bucket_name, 'processed/ieee-fraud-detection/')   # output used by Docker\n",
    "S3_OUTPUT_DATA_PATH = os.path.join(\"s3://\", bucket_name, 'output/ieee-fraud-detection/')   # output used by Docker\n",
    "\n",
    "S3_PREPROCESS_DATA_PATH, S3_OUTPUT_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66d191ef-7550-40b8-9b79-2653bcedfc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config created at ./processed/ieee-fraud-detection/config/config.json\n",
      "\n",
      "Note: Only 5 XGBoost hyperparameters are supported for GraphSAGE_XGBoost model:\n",
      "  - max_depth, learning_rate, num_parallel_tree, num_boost_round, gamma\n"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "    \"paths\": {\n",
    "        \"data_dir\": \"/opt/ml/input/data/gnn\",   # container mount path -> maps to PROCESSED_DATA_PATH on host\n",
    "        \"output_dir\": \"/opt/ml/model\"     # container path -> maps to MODEL_OUTPUT_PATH on host \n",
    "    },\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"kind\": \"GraphSAGE_XGBoost\",\n",
    "            \"gpu\": \"single\",\n",
    "            \"hyperparameters\": {\n",
    "                \"gnn\": {\n",
    "                    \"hidden_channels\": 32,\n",
    "                    \"n_hops\": 2,\n",
    "                    \"dropout_prob\": 0.2,\n",
    "                    \"batch_size\": 1024,\n",
    "                    \"fan_out\": 32,\n",
    "                    \"num_epochs\": 20\n",
    "                },\n",
    "                \"xgb\": {\n",
    "                    # Only these 5 parameters are allowed for GraphSAGE_XGBoost\n",
    "                    \"max_depth\": 8,\n",
    "                    \"learning_rate\": 0.1,\n",
    "                    \"num_parallel_tree\": 1,\n",
    "                    \"num_boost_round\": 1000,\n",
    "                    \"gamma\": 1.0\n",
    "                    # Removed unsupported params:\n",
    "                    # - tree_method (not allowed)\n",
    "                    # - enable_categorical (not allowed)\n",
    "                    # - min_child_weight (not allowed)\n",
    "                    # - subsample (not allowed)\n",
    "                    # - colsample_bytree (not allowed)\n",
    "                    # - scale_pos_weight (not allowed)\n",
    "                    # - eval_metric (not allowed)\n",
    "                    # - early_stopping_rounds (not allowed)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "config_path_local = os.path.join(PROCESSED_DATA_PATH, \"config\")\n",
    "config_path_filename = os.path.join(config_path_local, \"config.json\")\n",
    "!mkdir -p {config_path_local}\n",
    "\n",
    "with open(config_path_filename, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "    \n",
    "print(\"Training config created at\", config_path_filename)\n",
    "print(\"\\nNote: Only 5 XGBoost hyperparameters are supported for GraphSAGE_XGBoost model:\")\n",
    "print(\"  - max_depth, learning_rate, num_parallel_tree, num_boost_round, gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8df72ad1-35d6-4225-a9b1-865cc9f7a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded from ./processed/ieee-fraud-detection/config/config.json to: s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/config/config.json\n",
      "2025-10-19 02:54:32        594 processed/ieee-fraud-detection/config/config.json\n"
     ]
    }
   ],
   "source": [
    "s3_config_key = os.path.join(\"processed/ieee-fraud-detection/config/config.json\")\n",
    "upload_file(bucket_name, s3_config_key, config_path_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca867e14-08de-4321-b7b2-a07e4c628b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE edges/\n",
      "                           PRE nodes/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {S3_PREPROCESS_DATA_PATH}gnn/train_gnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52dd3e49-cf5d-4c51-b57e-965c243272ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrapper.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrapper.sh\n",
    "#!/bin/bash\n",
    "set -e\n",
    "set -x\n",
    "\n",
    "echo \"=== Starting wrapper script ===\"\n",
    "\n",
    "# Verify GPU is accessible\n",
    "echo \"=== Checking GPU availability ===\"\n",
    "nvidia-smi || echo \"WARNING: nvidia-smi failed\"\n",
    "python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA device count: {torch.cuda.device_count()}'); print(f'CUDA device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\\"N/A\\\"}')\"\n",
    "\n",
    "# Fix pyg-lib compatibility\n",
    "echo \"=== Fixing pyg-lib compatibility ===\"\n",
    "pip uninstall -y pyg-lib\n",
    "pip install --no-cache-dir pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
    "\n",
    "# Verify installation\n",
    "echo \"=== Verifying pyg-lib installation ===\"\n",
    "python -c \"import pyg_lib; print(f'pyg-lib version: {pyg_lib.__version__}')\"\n",
    "\n",
    "# Set up environment\n",
    "echo \"=== Setting up environment ===\"\n",
    "export PYTHONPATH=/opt/nim:${PYTHONPATH}\n",
    "\n",
    "# Warm up CUDA and verify GPU operations work\n",
    "echo \"=== Warming up CUDA ===\"\n",
    "python << 'WARMUP'\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    # Create a simple tensor operation to initialize CUDA context\n",
    "    x = torch.randn(100, 100, device=device)\n",
    "    y = torch.randn(100, 100, device=device)\n",
    "    z = torch.matmul(x, y)\n",
    "    print(f\"CUDA warmup successful. Device: {device}, Result shape: {z.shape}\")\n",
    "    # Force synchronization\n",
    "    torch.cuda.synchronize()\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available!\")\n",
    "WARMUP\n",
    "\n",
    "echo \"=== Inspecting /opt/ml recursively ===\"\n",
    "ls -R /opt/ml/\n",
    "\n",
    "# Create training launcher\n",
    "echo \"=== Creating training launcher ===\"\n",
    "cat > /tmp/launch_training.py << 'EOF'\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# Verify CUDA is available before training\n",
    "logging.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    logging.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "    # Initialize CUDA context\n",
    "    torch.cuda.init()\n",
    "    logging.info(\"CUDA context initialized\")\n",
    "\n",
    "# Add the library path\n",
    "sys.path.insert(0, '/opt/nim/lib/financial_fraud_training')\n",
    "\n",
    "# Load the config file\n",
    "config_path = '/opt/ml/input/data/config/config.json'\n",
    "logging.info(f\"Loading config from: {config_path}\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "logging.info(\"Config loaded successfully\")\n",
    "\n",
    "# Store training start time\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Import and call the training function\n",
    "    from src.validate_and_launch import validate_config_and_run_training\n",
    "    \n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"STARTING TRAINING\")\n",
    "    logging.info(\"=\" * 60)\n",
    "    \n",
    "    validate_config_and_run_training(config_dict)\n",
    "    \n",
    "    training_end_time = datetime.now()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    training_success = True\n",
    "    \n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    logging.info(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    training_end_time = datetime.now()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    training_success = False\n",
    "    \n",
    "    logging.error(\"=\" * 60)\n",
    "    logging.error(\"TRAINING FAILED\")\n",
    "    logging.error(\"=\" * 60)\n",
    "    logging.error(f\"Error: {e}\", exc_info=True)\n",
    "    \n",
    "finally:\n",
    "    # CREATE SNAPSHOT AFTER TRAINING (SUCCESS OR FAILURE)\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"CREATING TRAINING SNAPSHOT\")\n",
    "    logging.info(\"=\" * 60)\n",
    "    \n",
    "    snapshot_dir = '/opt/ml/model/training_snapshot'\n",
    "    os.makedirs(snapshot_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save training configuration\n",
    "    logging.info(\"Saving configuration...\")\n",
    "    shutil.copy(config_path, os.path.join(snapshot_dir, 'config.json'))\n",
    "    \n",
    "    # 2. Save INPUT DATA (all data channels)\n",
    "    logging.info(\"Saving input data snapshot...\")\n",
    "    input_data_dir = os.path.join(snapshot_dir, 'input_data')\n",
    "    os.makedirs(input_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy all data channels from /opt/ml/input/data/\n",
    "    sagemaker_data_dir = '/opt/ml/input/data'\n",
    "    if os.path.exists(sagemaker_data_dir):\n",
    "        for item in os.listdir(sagemaker_data_dir):\n",
    "            # Skip manifest files\n",
    "            if item.endswith('-manifest'):\n",
    "                continue\n",
    "            \n",
    "            src_path = os.path.join(sagemaker_data_dir, item)\n",
    "            dest_path = os.path.join(input_data_dir, item)\n",
    "            \n",
    "            try:\n",
    "                if os.path.isdir(src_path):\n",
    "                    # For directories, copy the entire tree\n",
    "                    shutil.copytree(src_path, dest_path, dirs_exist_ok=True)\n",
    "                    \n",
    "                    # Count files in this directory\n",
    "                    file_count = sum([len(files) for r, d, files in os.walk(dest_path)])\n",
    "                    dir_size = sum([os.path.getsize(os.path.join(r, f)) for r, d, files in os.walk(dest_path) for f in files])\n",
    "                    \n",
    "                    logging.info(f\"Copied input channel '{item}': {file_count} files, {dir_size / 1e6:.2f} MB\")\n",
    "                elif os.path.isfile(src_path):\n",
    "                    # For files, just copy\n",
    "                    shutil.copy2(src_path, dest_path)\n",
    "                    file_size = os.path.getsize(dest_path)\n",
    "                    logging.info(f\"Copied input file '{item}': {file_size / 1e6:.2f} MB\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not copy input data '{item}': {e}\")\n",
    "    \n",
    "    # 3. Save training metadata\n",
    "    logging.info(\"Saving training metadata...\")\n",
    "    metadata = {\n",
    "        'training_start': training_start_time.isoformat(),\n",
    "        'training_end': training_end_time.isoformat(),\n",
    "        'training_duration_seconds': training_duration.total_seconds(),\n",
    "        'training_success': training_success,\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        metadata['cuda_version'] = torch.version.cuda\n",
    "        metadata['gpu_name'] = torch.cuda.get_device_name(0)\n",
    "        metadata['gpu_memory_total_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        metadata['gpu_memory_allocated_gb'] = torch.cuda.memory_allocated(0) / 1e9\n",
    "        metadata['gpu_memory_reserved_gb'] = torch.cuda.memory_reserved(0) / 1e9\n",
    "    \n",
    "    with open(os.path.join(snapshot_dir, 'training_metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # 4. Save environment/package information\n",
    "    logging.info(\"Saving environment info...\")\n",
    "    result = subprocess.run(['pip', 'list', '--format=freeze'], capture_output=True, text=True)\n",
    "    with open(os.path.join(snapshot_dir, 'requirements.txt'), 'w') as f:\n",
    "        f.write(result.stdout)\n",
    "    \n",
    "    # 5. Save system information\n",
    "    logging.info(\"Saving system info...\")\n",
    "    with open(os.path.join(snapshot_dir, 'system_info.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Training Status: {'SUCCESS' if training_success else 'FAILED'}\\n\")\n",
    "        f.write(f\"Training Duration: {training_duration}\\n\")\n",
    "        f.write(f\"Start Time: {training_start_time}\\n\")\n",
    "        f.write(f\"End Time: {training_end_time}\\n\")\n",
    "        f.write(f\"\\n{'=' * 40}\\n\")\n",
    "        f.write(f\"PyTorch version: {torch.__version__}\\n\")\n",
    "        f.write(f\"CUDA available: {torch.cuda.is_available()}\\n\")\n",
    "        if torch.cuda.is_available():\n",
    "            f.write(f\"CUDA version: {torch.version.cuda}\\n\")\n",
    "            f.write(f\"GPU: {torch.cuda.get_device_name(0)}\\n\")\n",
    "            f.write(f\"GPU memory total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
    "            f.write(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\\n\")\n",
    "            f.write(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\\n\")\n",
    "    \n",
    "    # 6. Save source code snapshot\n",
    "    logging.info(\"Saving source code...\")\n",
    "    code_snapshot_dir = os.path.join(snapshot_dir, 'code')\n",
    "    os.makedirs(code_snapshot_dir, exist_ok=True)\n",
    "    \n",
    "    source_dirs = [\n",
    "        '/opt/nim/lib/financial_fraud_training/src',\n",
    "        '/opt/nim/lib/financial_fraud_training'\n",
    "    ]\n",
    "    \n",
    "    for src_dir in source_dirs:\n",
    "        if os.path.exists(src_dir):\n",
    "            dest_name = os.path.basename(src_dir)\n",
    "            dest_dir = os.path.join(code_snapshot_dir, dest_name)\n",
    "            try:\n",
    "                shutil.copytree(src_dir, dest_dir, dirs_exist_ok=True)\n",
    "                logging.info(f\"Copied {src_dir} to snapshot\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not copy {src_dir}: {e}\")\n",
    "    \n",
    "    # 7. List model artifacts that were created\n",
    "    logging.info(\"Cataloging model artifacts...\")\n",
    "    model_files = []\n",
    "    if os.path.exists('/opt/ml/model'):\n",
    "        for root, dirs, files in os.walk('/opt/ml/model'):\n",
    "            # Skip the snapshot directory itself to avoid recursion\n",
    "            if 'training_snapshot' in root:\n",
    "                continue\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                relative_path = os.path.relpath(file_path, '/opt/ml/model')\n",
    "                model_files.append({\n",
    "                    'path': relative_path,\n",
    "                    'size_bytes': file_size,\n",
    "                    'size_mb': file_size / 1e6\n",
    "                })\n",
    "    \n",
    "    with open(os.path.join(snapshot_dir, 'model_artifacts.json'), 'w') as f:\n",
    "        json.dump({'artifacts': model_files, 'total_files': len(model_files)}, f, indent=2)\n",
    "    \n",
    "    # 8. Create input data catalog\n",
    "    logging.info(\"Cataloging input data...\")\n",
    "    input_files = []\n",
    "    if os.path.exists(input_data_dir):\n",
    "        for root, dirs, files in os.walk(input_data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                relative_path = os.path.relpath(file_path, input_data_dir)\n",
    "                input_files.append({\n",
    "                    'path': relative_path,\n",
    "                    'size_bytes': file_size,\n",
    "                    'size_mb': file_size / 1e6\n",
    "                })\n",
    "    \n",
    "    with open(os.path.join(snapshot_dir, 'input_data_catalog.json'), 'w') as f:\n",
    "        json.dump({\n",
    "            'input_files': input_files, \n",
    "            'total_files': len(input_files),\n",
    "            'total_size_mb': sum(f['size_mb'] for f in input_files)\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    logging.info(f\"Training snapshot saved to {snapshot_dir}\")\n",
    "    logging.info(f\"Total model artifacts: {len(model_files)}\")\n",
    "    logging.info(f\"Total input files: {len(input_files)}\")\n",
    "    \n",
    "    # 9. Create a summary file \n",
    "    with open(os.path.join(snapshot_dir, 'SUMMARY.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"TRAINING SNAPSHOT SUMMARY\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Status: {'[SUCCESS]' if training_success else '[FAILED]'}\\n\")\n",
    "        f.write(f\"Duration: {training_duration}\\n\")\n",
    "        f.write(f\"Start: {training_start_time}\\n\")\n",
    "        f.write(f\"End: {training_end_time}\\n\\n\")\n",
    "        f.write(f\"Files in snapshot:\\n\")\n",
    "        f.write(f\"  - config.json (training configuration)\\n\")\n",
    "        f.write(f\"  - training_metadata.json (detailed metadata)\\n\")\n",
    "        f.write(f\"  - system_info.txt (system and GPU info)\\n\")\n",
    "        f.write(f\"  - requirements.txt (Python packages)\\n\")\n",
    "        f.write(f\"  - model_artifacts.json (list of model files)\\n\")\n",
    "        f.write(f\"  - input_data_catalog.json (list of input files)\\n\")\n",
    "        f.write(f\"  - code/ (source code snapshot)\\n\")\n",
    "        f.write(f\"  - input_data/ (all input data channels)\\n\")\n",
    "        f.write(f\"\\nTotal model artifacts: {len(model_files)}\\n\")\n",
    "        f.write(f\"Total input files: {len(input_files)}\\n\")\n",
    "        if input_files:\n",
    "            f.write(f\"Total input data size: {sum(f['size_mb'] for f in input_files):.2f} MB\\n\")\n",
    "    \n",
    "    logging.info(\"Snapshot creation completed\")\n",
    "    \n",
    "    # Re-raise exception if training failed\n",
    "    if not training_success:\n",
    "        raise\n",
    "\n",
    "EOF\n",
    "\n",
    "echo \"=== Inspecting config file ===\"\n",
    "cat /opt/ml/input/data/config/config.json\n",
    "\n",
    "# Run training with our launcher\n",
    "echo \"=== Starting training ===\"\n",
    "torchrun --standalone --nproc_per_node=1 /tmp/launch_training.py\n",
    "\n",
    "# List output directory after everything completes\n",
    "echo \"=== Final model directory contents ===\"\n",
    "ls -lah /opt/ml/model/\n",
    "echo \"\"\n",
    "echo \"=== Snapshot contents ===\"\n",
    "ls -lah /opt/ml/model/training_snapshot/\n",
    "echo \"\"\n",
    "echo \"=== Input data snapshot ===\"\n",
    "ls -lah /opt/ml/model/training_snapshot/input_data/ 2>/dev/null || echo \"No input data found\"\n",
    "echo \"\"\n",
    "echo \"=== Snapshot summary ===\"\n",
    "cat /opt/ml/model/training_snapshot/SUMMARY.txt 2>/dev/null || echo \"No summary file found\"\n",
    "\n",
    "echo \"=== Wrapper script completed ===\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a7cac32-bb0f-4843-a8e4-317b34884c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper script uploaded to: s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/scripts/wrapper.sh\n",
      "File uploaded from wrapper.sh to: s3://sagemaker-us-east-1-176843580427/processed/ieee-fraud-detection/scripts/wrapper.sh\n",
      "2025-10-19 04:23:13      12791 processed/ieee-fraud-detection/scripts/wrapper.sh\n"
     ]
    }
   ],
   "source": [
    "# Upload wrapper script to S3\n",
    "s3_wrapper_key = os.path.join(\"processed/ieee-fraud-detection/scripts/wrapper.sh\")\n",
    "\n",
    "wrapper_s3_path = f\"s3://{bucket_name}/{s3_wrapper_key}\"\n",
    "print(f\"Wrapper script uploaded to: {wrapper_s3_path}\")\n",
    "\n",
    "upload_file(bucket_name, s3_wrapper_key, 'wrapper.sh')\n",
    "\n",
    "# !aws s3 ls {S3_PREPROCESS_DATA_PATH} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a50f22c-cf55-4616-b9ff-7b4b9926a13f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:Framework profiling will be deprecated from tensorflow 2.12 and pytorch 2.0 in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: fraud-detection-gnn-19-Oct-2025-04-23-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 04:23:14 Starting - Starting the training job\n",
      "2025-10-19 04:23:14 Pending - Training job waiting for capacity...\n",
      "2025-10-19 04:23:45 Pending - Preparing the instances for training...\n",
      "2025-10-19 04:24:13 Downloading - Downloading input data.........\n",
      "2025-10-19 04:25:39 Downloading - Downloading the training image.............................................\n",
      "2025-10-19 04:33:03 Training - Training image download completed. Training in progress.\u001b[34m+ echo '=== Starting wrapper script ==='\u001b[0m\n",
      "\u001b[34m+ echo '=== Checking GPU availability ==='\u001b[0m\n",
      "\u001b[34m+ nvidia-smi\u001b[0m\n",
      "\u001b[34m=== Starting wrapper script ===\u001b[0m\n",
      "\u001b[34m=== Checking GPU availability ===\u001b[0m\n",
      "\u001b[34mSun Oct 19 04:33:15 2025       \u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\u001b[0m\n",
      "\u001b[34m|-----------------------------------------+------------------------+----------------------+\u001b[0m\n",
      "\u001b[34m| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\u001b[0m\n",
      "\u001b[34m| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\u001b[0m\n",
      "\u001b[34m|                                         |                        |               MIG M. |\u001b[0m\n",
      "\u001b[34m|=========================================+========================+======================|\u001b[0m\n",
      "\u001b[34m|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\u001b[0m\n",
      "\u001b[34m|  0%   22C    P8              9W /  300W |       0MiB /  23028MiB |      0%      Default |\u001b[0m\n",
      "\u001b[34m|                                         |                        |                  N/A |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m| Processes:                                                                              |\u001b[0m\n",
      "\u001b[34m|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\u001b[0m\n",
      "\u001b[34m|        ID   ID                                                               Usage      |\u001b[0m\n",
      "\u001b[34m|=========================================================================================|\u001b[0m\n",
      "\u001b[34m|  No running processes found                                                             |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m+ python -c 'import torch; print(f'\\''CUDA available: {torch.cuda.is_available()}'\\''); print(f'\\''CUDA device count: {torch.cuda.device_count()}'\\''); print(f'\\''CUDA device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}'\\'')'\u001b[0m\n",
      "\u001b[34mCUDA available: True\u001b[0m\n",
      "\u001b[34mCUDA device count: 1\u001b[0m\n",
      "\u001b[34mCUDA device name: NVIDIA A10G\u001b[0m\n",
      "\u001b[34m+ echo '=== Fixing pyg-lib compatibility ==='\u001b[0m\n",
      "\u001b[34m+ pip uninstall -y pyg-lib\u001b[0m\n",
      "\u001b[34m=== Fixing pyg-lib compatibility ===\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/faiss-1.7.4-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pyg_lib-0.4.0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mFound existing installation: pyg_lib 0.4.0\u001b[0m\n",
      "\u001b[34mUninstalling pyg_lib-0.4.0:\u001b[0m\n",
      "\u001b[34m  Successfully uninstalled pyg_lib-0.4.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m+ pip install --no-cache-dir pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/faiss-1.7.4-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://urm.nvidia.com/artifactory/api/pypi/nv-shared-pypi/simple, https://pypi.ngc.nvidia.com\u001b[0m\n",
      "\u001b[34mLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\u001b[0m\n",
      "\u001b[34mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f7885abecf0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': /artifactory/api/pypi/nv-shared-pypi/simple/pyg-lib/\u001b[0m\n",
      "\u001b[34mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f7885abf1d0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': /artifactory/api/pypi/nv-shared-pypi/simple/pyg-lib/\u001b[0m\n",
      "\u001b[34mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f7885abf260>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': /artifactory/api/pypi/nv-shared-pypi/simple/pyg-lib/\u001b[0m\n",
      "\u001b[34mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f7885abf4a0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': /artifactory/api/pypi/nv-shared-pypi/simple/pyg-lib/\u001b[0m\n",
      "\u001b[34mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f7885abf6e0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': /artifactory/api/pypi/nv-shared-pypi/simple/pyg-lib/\u001b[0m\n",
      "\u001b[34mCollecting pyg-lib\n",
      "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.5.0%2Bpt26cu124-cp312-cp312-linux_x86_64.whl (4.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 355.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyg-lib\u001b[0m\n",
      "\u001b[34mSuccessfully installed pyg-lib-0.5.0+pt26cu124\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m=== Verifying pyg-lib installation ===\u001b[0m\n",
      "\u001b[34m+ echo '=== Verifying pyg-lib installation ==='\u001b[0m\n",
      "\u001b[34m+ python -c 'import pyg_lib; print(f'\\''pyg-lib version: {pyg_lib.__version__}'\\'')'\u001b[0m\n",
      "\u001b[34mpyg-lib version: 0.5.0+pt26cu124\u001b[0m\n",
      "\u001b[34m+ echo '=== Setting up environment ==='\u001b[0m\n",
      "\u001b[34m+ export PYTHONPATH=/opt/nim:\u001b[0m\n",
      "\u001b[34m+ PYTHONPATH=/opt/nim:\u001b[0m\n",
      "\u001b[34m+ echo '=== Warming up CUDA ==='\u001b[0m\n",
      "\u001b[34m+ python\u001b[0m\n",
      "\u001b[34m=== Setting up environment ===\u001b[0m\n",
      "\u001b[34m=== Warming up CUDA ===\u001b[0m\n",
      "\u001b[34mCUDA warmup successful. Device: cuda:0, Result shape: torch.Size([100, 100])\u001b[0m\n",
      "\u001b[34m+ echo '=== Inspecting /opt/ml recursively ==='\u001b[0m\n",
      "\u001b[34m+ ls -R /opt/ml/\u001b[0m\n",
      "\u001b[34m=== Inspecting /opt/ml recursively ===\u001b[0m\n",
      "\u001b[34m/opt/ml/:\u001b[0m\n",
      "\u001b[34minput\u001b[0m\n",
      "\u001b[34mmodel\u001b[0m\n",
      "\u001b[34moutput\u001b[0m\n",
      "\u001b[34msagemaker\u001b[0m\n",
      "\u001b[34m/opt/ml/input:\u001b[0m\n",
      "\u001b[34mconfig\u001b[0m\n",
      "\u001b[34mdata\u001b[0m\n",
      "\u001b[34m/opt/ml/input/config:\u001b[0m\n",
      "\u001b[34mdebughookconfig.json\u001b[0m\n",
      "\u001b[34mhyperparameters.json\u001b[0m\n",
      "\u001b[34minit-config.json\u001b[0m\n",
      "\u001b[34minputdataconfig.json\u001b[0m\n",
      "\u001b[34mmetric-definition-regex.json\u001b[0m\n",
      "\u001b[34mprofilerconfig.json\u001b[0m\n",
      "\u001b[34mresourceconfig.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data:\u001b[0m\n",
      "\u001b[34mconfig\u001b[0m\n",
      "\u001b[34m+ echo '=== Creating training launcher ==='\u001b[0m\n",
      "\u001b[34mconfig-manifest\u001b[0m\n",
      "\u001b[34mgnn\u001b[0m\n",
      "\u001b[34mgnn-manifest\u001b[0m\n",
      "\u001b[34mscripts\u001b[0m\n",
      "\u001b[34mscripts-manifest\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/config:\u001b[0m\n",
      "\u001b[34mconfig.json\u001b[0m\n",
      "\u001b[34mtraining_config.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/gnn:\u001b[0m\n",
      "\u001b[34medges\u001b[0m\n",
      "\u001b[34mnodes\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/gnn/edges:\u001b[0m\n",
      "\u001b[34mnode_to_node.csv\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/gnn/nodes:\u001b[0m\n",
      "\u001b[34mnode.csv\u001b[0m\n",
      "\u001b[34mnode_label.csv\u001b[0m\n",
      "\u001b[34m+ cat\u001b[0m\n",
      "\u001b[34moffset_range_of_training_node.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/scripts:\u001b[0m\n",
      "\u001b[34mwrapper.sh\u001b[0m\n",
      "\u001b[34m/opt/ml/model:\u001b[0m\n",
      "\u001b[34m/opt/ml/output:\u001b[0m\n",
      "\u001b[34mdata\u001b[0m\n",
      "\u001b[34mmetrics\u001b[0m\n",
      "\u001b[34mprofiler\u001b[0m\n",
      "\u001b[34mtensors\u001b[0m\n",
      "\u001b[34m/opt/ml/output/data:\u001b[0m\n",
      "\u001b[34m/opt/ml/output/metrics:\u001b[0m\n",
      "\u001b[34msagemaker\u001b[0m\n",
      "\u001b[34m/opt/ml/output/metrics/sagemaker:\u001b[0m\n",
      "\u001b[34m/opt/ml/output/profiler:\u001b[0m\n",
      "\u001b[34mframework\u001b[0m\n",
      "\u001b[34m/opt/ml/output/profiler/framework:\u001b[0m\n",
      "\u001b[34m/opt/ml/output/tensors:\u001b[0m\n",
      "\u001b[34m/opt/ml/sagemaker:\u001b[0m\n",
      "\u001b[34mssm\u001b[0m\n",
      "\u001b[34mwarmpoolcache\u001b[0m\n",
      "\u001b[34m/opt/ml/sagemaker/ssm:\u001b[0m\n",
      "\u001b[34m/opt/ml/sagemaker/warmpoolcache:\u001b[0m\n",
      "\u001b[34m=== Creating training launcher ===\u001b[0m\n",
      "\u001b[34m+ echo '=== Inspecting config file ==='\u001b[0m\n",
      "\u001b[34m+ cat /opt/ml/input/data/config/config.json\u001b[0m\n",
      "\u001b[34m=== Inspecting config file ===\u001b[0m\n",
      "\u001b[34m{\n",
      "  \"paths\": {\n",
      "    \"data_dir\": \"/opt/ml/input/data/gnn\",\n",
      "    \"output_dir\": \"/opt/ml/model\"\n",
      "  },\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"kind\": \"GraphSAGE_XGBoost\",\u001b[0m\n",
      "\u001b[34m+ echo '=== Starting training ==='\u001b[0m\n",
      "\u001b[34m+ torchrun --standalone --nproc_per_node=1 /tmp/launch_training.py\n",
      "      \"gpu\": \"single\",\n",
      "      \"hyperparameters\": {\n",
      "        \"gnn\": {\n",
      "          \"hidden_channels\": 32,\n",
      "          \"n_hops\": 2,\n",
      "          \"dropout_prob\": 0.2,\n",
      "          \"batch_size\": 1024,\n",
      "          \"fan_out\": 32,\n",
      "          \"num_epochs\": 20\n",
      "        },\n",
      "        \"xgb\": {\n",
      "          \"max_depth\": 8,\n",
      "          \"learning_rate\": 0.1,\n",
      "          \"num_parallel_tree\": 1,\n",
      "          \"num_boost_round\": 1000,\n",
      "          \"gamma\": 1.0\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\u001b[0m\n",
      "\u001b[34m}=== Starting training ===\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:36 [INFO] CUDA available: True\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:36 [INFO] CUDA device: NVIDIA A10G\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:36 [INFO] CUDA version: 12.4\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:36 [INFO] CUDA context initialized\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:36 [INFO] Loading config from: /opt/ml/input/data/config/config.json\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:36 [INFO] Config loaded successfully\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:43 [INFO] ============================================================\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:43 [INFO] STARTING TRAINING\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:43 [INFO] ============================================================\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:43 [INFO] The provided training configuration has been successfully validated.\u001b[0m\n",
      "\u001b[34mget_mempolicy: Operation not permitted\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] init\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Loaded node type 'node': 413378 nodes, feature dim 434 (id: 0).\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Loaded labels for node type 'node' from '/opt/ml/input/data/gnn/nodes/node_label.csv'.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Node meta mapping: {'node': {'id': 0, 'num_nodes': 413378, 'feat_dim': 434}}\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Loaded edge type ('node', 'to', 'node'): 1653512 edges.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Edge meta mapping: {('node', 'to', 'node'): {'num_edges': 1653512, 'attr_dim': 0}}\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Meta file written to 'meta.json'.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Node feature matrix shape: torch.Size([413378, 434])\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Processed labels for node type 'node' with 413378 entries.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Homogeneous node label tensor shape: torch.Size([413378, 1])\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Node label tensor shape: torch.Size([413378, 1])\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Offset for node type 'node' (id 0): 0\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Processed edge type ('node', 'to', 'node') with 1653512 edges.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Homogeneous edge index shape: torch.Size([2, 1653512])\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] edge index shape: torch.Size([2, 1653512])\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] Data object is valid: x.shape=(413378, 434), y.shape=(413378, 1)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.12/dist-packages/cugraph_pyg/data/feature_store.py:67: UserWarning: Ignoring index parameter (attribute does not exist for group node)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:51 [INFO] -----Training XGBoost on embeddings produced by GraphSAGE model-----\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:54 [INFO] Epoch 0, validation f1 : 0.6500356379187455, took 2.00 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:55 [INFO] Epoch 1, validation f1 : 0.8426262920366866, took 1.59 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:57 [INFO] Epoch 2, validation f1 : 0.9270166453265045, took 1.60 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:33:58 [INFO] Epoch 3, validation f1 : 0.9953584321815369, took 1.59 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:00 [INFO] Epoch 4, validation f1 : 0.9958720330237358, took 1.60 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:01 [INFO] Epoch 5, validation f1 : 0.9972442301067861, took 1.62 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:03 [INFO] Epoch 6, validation f1 : 0.998448543354594, took 1.60 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:05 [INFO] Epoch 7, validation f1 : 0.9993093922651933, took 1.59 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:06 [INFO] Epoch 8, validation f1 : 0.9933024214322514, took 1.60 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:08 [INFO] Epoch 9, validation f1 : 0.9698492462311558, took 1.59 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:09 [INFO] Epoch 10, validation f1 : 0.9993093922651933, took 1.59 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:17 [INFO] Training on validation data took 7.70 seconds\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Training xgboost on embeddings took 7.49 seconds\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Elapsed time for data loading and training: 41.4975 seconds\u001b[0m\n",
      "\u001b[34m/opt/nim/lib/financial_fraud_training/src/gnn_models.py:69: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if return_hidden:\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] ------Saving ONNX model repository in /opt/ml/model/model_repository-----\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved GraphSAGE model to /opt/ml/model/model_repository/model/1/graph_sage_node_embedder.onnx\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved XGBoost model to /opt/ml/model/model_repository/xgboost/1/xgboost_on_embeddings.json\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved GraphSAGE model config to /opt/ml/model/model_repository/model/config.pbtxt\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved XGBoost model config to /opt/ml/model/model_repository/xgboost/config.pbtxt\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] ------Saving model repository for python backend in /opt/ml/model/python_backend_model_repository-----\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved GraphSAGE model state dict to /opt/ml/model/python_backend_model_repository/prediction_and_shapley/1/state_dict_gnn_model.pth\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved XGBoost model to /opt/ml/model/python_backend_model_repository/prediction_and_shapley/1/embedding_based_xgboost.json\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saved GraphSAGE model config to /opt/ml/model/python_backend_model_repository/prediction_and_shapley/config.pbtxt\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] ============================================================\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] TRAINING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] ============================================================\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] ============================================================\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] CREATING TRAINING SNAPSHOT\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] ============================================================\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saving configuration...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Saving input data snapshot...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Copied input channel 'config': 3 files, 0.00 MB\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:25 [INFO] Copied input channel 'scripts': 1 files, 0.01 MB\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Copied input channel 'gnn': 4 files, 2446.57 MB\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Saving training metadata...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Saving environment info...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Saving system info...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Saving source code...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Copied /opt/nim/lib/financial_fraud_training/src to snapshot\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Copied /opt/nim/lib/financial_fraud_training to snapshot\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Cataloging model artifacts...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Cataloging input data...\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Training snapshot saved to /opt/ml/model/training_snapshot\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Total model artifacts: 8\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Total input files: 8\u001b[0m\n",
      "\u001b[34m2025-10-19 04:34:29 [INFO] Snapshot creation completed\u001b[0m\n",
      "\u001b[34m+ echo '=== Final model directory contents ==='\u001b[0m\n",
      "\u001b[34m+ ls -lah /opt/ml/model/\u001b[0m\n",
      "\u001b[34m=== Final model directory contents ===\u001b[0m\n",
      "\u001b[34mtotal 16K\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 5 root root 4.0K Oct 19 04:34 .\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 6 root root   63 Oct 19 04:33 ..\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 4 root root 4.0K Oct 19 04:34 model_repository\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 3 root root 4.0K Oct 19 04:34 python_backend_model_repository\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 4 root root 4.0K Oct 19 04:34 training_snapshot\u001b[0m\n",
      "\u001b[34m+ echo ''\u001b[0m\n",
      "\u001b[34m+ echo '=== Snapshot contents ==='\u001b[0m\n",
      "\u001b[34m+ ls -lah /opt/ml/model/training_snapshot/\u001b[0m\n",
      "\u001b[34m=== Snapshot contents ===\u001b[0m\n",
      "\u001b[34mtotal 48K\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 4 root root 4.0K Oct 19 04:34 .\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 5 root root 4.0K Oct 19 04:34 ..\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  700 Oct 19 04:34 SUMMARY.txt\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 4 root root 4.0K Oct 19 04:34 code\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  594 Oct 19 04:34 config.json\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 5 root root 4.0K Oct 19 04:34 input_data\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  975 Oct 19 04:34 input_data_catalog.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root 1.2K Oct 19 04:34 model_artifacts.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root 7.5K Oct 19 04:34 requirements.txt\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  350 Oct 19 04:34 system_info.txt\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  410 Oct 19 04:34 training_metadata.json\u001b[0m\n",
      "\u001b[34m+ echo ''\u001b[0m\n",
      "\u001b[34m+ echo '=== Input data snapshot ==='\u001b[0m\n",
      "\u001b[34m+ ls -lah /opt/ml/model/training_snapshot/input_data/\u001b[0m\n",
      "\u001b[34m=== Input data snapshot ===\u001b[0m\n",
      "\u001b[34mtotal 20K\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 5 root root 4.0K Oct 19 04:34 .\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 4 root root 4.0K Oct 19 04:34 ..\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 3 root root 4.0K Oct 19 04:24 config\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 4 root root 4.0K Oct 19 04:24 gnn\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 2 root root 4.0K Oct 19 04:24 scripts\u001b[0m\n",
      "\u001b[34m+ echo ''\u001b[0m\n",
      "\u001b[34m=== Snapshot summary ===\u001b[0m\n",
      "\u001b[34m+ echo '=== Snapshot summary ==='\u001b[0m\n",
      "\u001b[34m+ cat /opt/ml/model/training_snapshot/SUMMARY.txt\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34mTRAINING SNAPSHOT SUMMARY\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34mStatus: [SUCCESS]\u001b[0m\n",
      "\u001b[34mDuration: 0:00:48.999265\u001b[0m\n",
      "\u001b[34mStart: 2025-10-19 04:33:36.488819\u001b[0m\n",
      "\u001b[34mEnd: 2025-10-19 04:34:25.488084\u001b[0m\n",
      "\u001b[34mFiles in snapshot:\n",
      "  - config.json (training configuration)\n",
      "  - training_metadata.json (detailed metadata)\n",
      "  - system_info.txt (system and GPU info)\n",
      "  - requirements.txt (Python packages)\u001b[0m\n",
      "\u001b[34m+ echo '=== Wrapper script completed ==='\n",
      "  - model_artifacts.json (list of model files)\n",
      "  - input_data_catalog.json (list of input files)\n",
      "  - code/ (source code snapshot)\n",
      "  - input_data/ (all input data channels)\u001b[0m\n",
      "\u001b[34mTotal model artifacts: 8\u001b[0m\n",
      "\u001b[34mTotal input files: 8\u001b[0m\n",
      "\u001b[34mTotal input data size: 2446.58 MB\u001b[0m\n",
      "\u001b[34m=== Wrapper script completed ===\u001b[0m\n",
      "\n",
      "2025-10-19 04:34:39 Uploading - Uploading generated training model\n",
      "2025-10-19 04:35:02 Completed - Training job completed\n",
      "Training seconds: 648\n",
      "Billable seconds: 648\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%d-%b-%Y-%H-%M-%S\")\n",
    "# Create Estimator object using SageMaker SDK\n",
    "estimator = Estimator(\n",
    "    image_uri=ecr_repo_name,\n",
    "    role=sagemaker_training_role,\n",
    "    instance_count=1,\n",
    "    # instance_type=\"ml.g4dn.2xlarge\", # The T4 GPU (16GB) rans out of memory\n",
    "    instance_type=\"ml.g5.xlarge\",  # 24GB GPU (A10G) instead of ml.g4dn.2xlarge (16GB T4) but increase quota at https://176843580427-dzcgepfa.us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas/L-B6D80D9C. More info at https://aws.amazon.com/ec2/instance-types/g5/\n",
    "    # OR\n",
    "    # instance_type=\"ml.g5.2xlarge\",  # 24GB GPU\n",
    "    # OR  \n",
    "    # instance_type=\"ml.p3.2xlarge\",  # 16GB V100 (might be enough with memory cleanup)\n",
    "    volume_size=30,\n",
    "    max_run=86400,\n",
    "    base_job_name=\"fraud-detection-gnn\",\n",
    "    output_path=S3_OUTPUT_DATA_PATH,\n",
    "    sagemaker_session=session,\n",
    "    # For 1.0.1\n",
    "    container_entry_point=[\n",
    "        \"bash\",\n",
    "        \"/opt/ml/input/data/scripts/wrapper.sh\"\n",
    "    ],\n",
    "    # container_log_level=logging.DEBUG,\n",
    "    # https://docs.nvidia.com/nim/financial-fraud-training/1.0.0/getting-started/cont-running.html\n",
    "    environment={\n",
    "        \"NIM_DISABLE_MODEL_DOWNLOAD\": \"true\",\n",
    "        \"NGC_API_KEY\": NGC_API_KEY,\n",
    "        \"PYTHONUNBUFFERED\": \"1\"  # helpful for real-time logs\n",
    "    },\n",
    "    # Enable profiling for GPU metrics\n",
    "    profiler_config=ProfilerConfig(\n",
    "        system_monitor_interval_millis=500,  # Collect every 500ms\n",
    "        framework_profile_params=FrameworkProfile(\n",
    "            detailed_profiling_config=DetailedProfilingConfig(\n",
    "                start_step=0,\n",
    "                num_steps=10\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define input channels (S3 URIs)\n",
    "inputs = {\n",
    "    \"gnn\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=os.path.join(S3_PREPROCESS_DATA_PATH, \"gnn/train_gnn/\"),\n",
    "        content_type=\"application/x-directory\",\n",
    "        input_mode=\"File\"\n",
    "    ),\n",
    "    \"config\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=os.path.join(S3_PREPROCESS_DATA_PATH, \"config\"),\n",
    "        content_type=\"application/x-directory\",\n",
    "        input_mode=\"File\"\n",
    "    ),\n",
    "    \"scripts\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=wrapper_s3_path,\n",
    "        content_type=\"text/x-sh\",\n",
    "        input_mode=\"File\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Launch the training job and track in notebook\n",
    "estimator.fit(\n",
    "    inputs=inputs,\n",
    "    job_name=f\"fraud-detection-gnn-{timestamp}\",\n",
    "    logs=[\"All\"],  # stream logs directly to the notebook\n",
    "    wait=True   # wait for job completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b6774e0-2439-4a81-a699-0d35466ac664",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 04:35:00          0 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/debug-output/training_job_end.ts\n",
      "2025-10-19 04:34:57  269508998 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/output/model.tar.gz\n",
      "2025-10-19 04:35:00          0 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/framework/training_job_end.ts\n",
      "2025-10-19 04:26:01     209439 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760847840.algo-1.json\n",
      "2025-10-19 04:26:00     272836 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760847900.algo-1.json\n",
      "2025-10-19 04:27:00     272284 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760847960.algo-1.json\n",
      "2025-10-19 04:28:00     271807 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848020.algo-1.json\n",
      "2025-10-19 04:29:00     271813 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848080.algo-1.json\n",
      "2025-10-19 04:30:00     271912 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848140.algo-1.json\n",
      "2025-10-19 04:31:00     272033 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848200.algo-1.json\n",
      "2025-10-19 04:32:00     271901 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848260.algo-1.json\n",
      "2025-10-19 04:33:00     271963 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848320.algo-1.json\n",
      "2025-10-19 04:34:42     268016 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848380.algo-1.json\n",
      "2025-10-19 04:34:42     188311 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/incremental/2025101904/1760848440.algo-1.json\n",
      "2025-10-19 04:35:00          0 output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/profiler-output/system/training_job_end.ts\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive {estimator.output_path}{estimator._current_job_name}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ee53430-f194-4112-bc9e-c1472f96d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-176843580427/output/ieee-fraud-detection/fraud-detection-gnn-19-Oct-2025-04-23-13/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {estimator.output_path}{estimator._current_job_name}/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcd3aa21-b76b-46ab-b23f-3589bd0372fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./output’: File exists\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/prediction_and_shapley/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/prediction_and_shapley/1/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/prediction_and_shapley/1/state_dict_gnn_model.pth\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/prediction_and_shapley/1/model.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/prediction_and_shapley/1/embedding_based_xgboost.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "python_backend_model_repository/prediction_and_shapley/config.pbtxt\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/model/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/model/1/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/model/1/graph_sage_node_embedder.onnx\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/model/config.pbtxt\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/xgboost/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/xgboost/1/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/xgboost/1/xgboost_on_embeddings.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_repository/xgboost/config.pbtxt\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/training_metadata.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data_catalog.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/requirements.txt\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/config/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/config/.ipynb_checkpoints/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/config/.ipynb_checkpoints/config-checkpoint.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/config/training_config.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/config/config.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/scripts/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/scripts/wrapper.sh\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/edges/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/edges/node_to_node.csv\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/nodes/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/nodes/node.csv\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/nodes/offset_range_of_training_node.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/input_data/gnn/nodes/node_label.csv\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/system_info.txt\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/model_artifacts.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/validate_and_launch.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/gnn_models.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/train_xgboost.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/__init__.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/config_schema.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/src/train_gnn_based_xgboost.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/example_training_config_graphsage_xgboost.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/tests/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/tests/test_gnn_based_xgboost_training.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/tests/test_standalone_xgboost_training.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/validate_and_launch.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/gnn_models.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/train_xgboost.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/__init__.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/config_schema.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/src/train_gnn_based_xgboost.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/main.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/README.md\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/example_training_config_xgboost.json\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/data/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/data/data_def.md\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/data/index.rst\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/conf.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/index.rst\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/configuration/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/configuration/config_json.md\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/configuration/index.rst\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/make.bat\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/Makefile\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/__init__.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/utils/\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/utils/triton_model_repo_generator.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/utils/python_backend_model.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/utils/graph_data_reader.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/utils/__init__.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/utils/proto_text_generator.py\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/code/financial_fraud_training/Dockerfile\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/SUMMARY.txt\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "training_snapshot/config.json\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./output\n",
    "!tar -xvzf ./model.tar.gz -C ./output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e38f17a-0410-4955-9e44-0b394155ceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SNAPSHOT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Status: [SUCCESS]\n",
      "Duration: 0:00:48.999265\n",
      "Start: 2025-10-19 04:33:36.488819\n",
      "End: 2025-10-19 04:34:25.488084\n",
      "\n",
      "Files in snapshot:\n",
      "  - config.json (training configuration)\n",
      "  - training_metadata.json (detailed metadata)\n",
      "  - system_info.txt (system and GPU info)\n",
      "  - requirements.txt (Python packages)\n",
      "  - model_artifacts.json (list of model files)\n",
      "  - input_data_catalog.json (list of input files)\n",
      "  - code/ (source code snapshot)\n",
      "  - input_data/ (all input data channels)\n",
      "\n",
      "Total model artifacts: 8\n",
      "Total input files: 8\n",
      "Total input data size: 2446.58 MB\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/training_snapshot/SUMMARY.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da1e9d5d-ae86-49e6-a650-9a7715877a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/:\n",
      "model_repository  python_backend_model_repository  training_snapshot\n",
      "\n",
      "./output/model_repository:\n",
      "model  xgboost\n",
      "\n",
      "./output/model_repository/model:\n",
      "1  config.pbtxt\n",
      "\n",
      "./output/model_repository/model/1:\n",
      "graph_sage_node_embedder.onnx\n",
      "\n",
      "./output/model_repository/xgboost:\n",
      "1  config.pbtxt\n",
      "\n",
      "./output/model_repository/xgboost/1:\n",
      "xgboost_on_embeddings.json\n",
      "\n",
      "./output/python_backend_model_repository:\n",
      "prediction_and_shapley\n",
      "\n",
      "./output/python_backend_model_repository/prediction_and_shapley:\n",
      "1  config.pbtxt\n",
      "\n",
      "./output/python_backend_model_repository/prediction_and_shapley/1:\n",
      "embedding_based_xgboost.json  model.py\tstate_dict_gnn_model.pth\n",
      "\n",
      "./output/training_snapshot:\n",
      "code\t     input_data_catalog.json  SUMMARY.txt\n",
      "config.json  model_artifacts.json     system_info.txt\n",
      "input_data   requirements.txt\t      training_metadata.json\n",
      "\n",
      "./output/training_snapshot/code:\n",
      "financial_fraud_training  src\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training:\n",
      "Dockerfile\t\t\t\t\t__init__.py  tests\n",
      "docs\t\t\t\t\t\tmain.py      utils\n",
      "example_training_config_graphsage_xgboost.json\tREADME.md\n",
      "example_training_config_xgboost.json\t\tsrc\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/docs:\n",
      "financial_fraud_training\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/docs/financial_fraud_training:\n",
      "make.bat  Makefile  source\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source:\n",
      "configuration  conf.py\tdata  index.rst\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/configuration:\n",
      "config_json.md\tindex.rst\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/docs/financial_fraud_training/source/data:\n",
      "data_def.md  index.rst\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/src:\n",
      "config_schema.py  __init__.py\t\t      train_xgboost.py\n",
      "gnn_models.py\t  train_gnn_based_xgboost.py  validate_and_launch.py\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/tests:\n",
      "test_gnn_based_xgboost_training.py  test_standalone_xgboost_training.py\n",
      "\n",
      "./output/training_snapshot/code/financial_fraud_training/utils:\n",
      "graph_data_reader.py  proto_text_generator.py  triton_model_repo_generator.py\n",
      "__init__.py\t      python_backend_model.py\n",
      "\n",
      "./output/training_snapshot/code/src:\n",
      "config_schema.py  __init__.py\t\t      train_xgboost.py\n",
      "gnn_models.py\t  train_gnn_based_xgboost.py  validate_and_launch.py\n",
      "\n",
      "./output/training_snapshot/input_data:\n",
      "config\tgnn  scripts\n",
      "\n",
      "./output/training_snapshot/input_data/config:\n",
      "config.json  training_config.json\n",
      "\n",
      "./output/training_snapshot/input_data/gnn:\n",
      "edges  nodes\n",
      "\n",
      "./output/training_snapshot/input_data/gnn/edges:\n",
      "node_to_node.csv\n",
      "\n",
      "./output/training_snapshot/input_data/gnn/nodes:\n",
      "node.csv  node_label.csv  offset_range_of_training_node.json\n",
      "\n",
      "./output/training_snapshot/input_data/scripts:\n",
      "wrapper.sh\n"
     ]
    }
   ],
   "source": [
    "!ls -R ./output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890bf69-be59-4a05-ac8b-cce554b6f0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
